{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pesco.eval import load_evaluator_from_file\n",
    "\n",
    "evaluator = load_evaluator_from_file(\"../data/cpachecker_labels.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pesco.utils import load_dataset\n",
    "\n",
    "CST_FEATURES = \"../data/datasets/svcomp22_cst_tool.jsonl\"\n",
    "LABELS = \"../data/cpachecker_labels.jsonl\"\n",
    "\n",
    "cst_dataset = load_dataset(CST_FEATURES, LABELS, fill_unknown = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4602\n"
     ]
    }
   ],
   "source": [
    "# Identify trivial instances\n",
    "nontrivial = cst_dataset.labels.max(axis = 1) !=  0\n",
    "nontrivial = set(instance for i, instance in enumerate(cst_dataset.instance_index) if nontrivial[i])\n",
    "print(len(nontrivial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4602, 128)\n"
     ]
    }
   ],
   "source": [
    "common_instances = list(sorted(nontrivial))\n",
    "\n",
    "from pesco.utils import Dataset, _index_elements\n",
    "\n",
    "cst_index = {k: i for i, k in enumerate(cst_dataset.instance_index)}\n",
    "cst_indices = [cst_index[k] for k in common_instances]\n",
    "cst_dataset  = Dataset(*map(lambda x: _index_elements(x, cst_indices), cst_dataset[:-1]), cst_dataset.label_index)\n",
    "\n",
    "print(cst_dataset.embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3254, 128), (1348, 128))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pesco.utils import stratified_split\n",
    "\n",
    "train_dataset, test_dataset = stratified_split(cst_dataset, 0.3)\n",
    "train_dataset.embedding.shape, test_dataset.embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import  PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "n_components = 128\n",
    "lsa = make_pipeline(StandardScaler(), PCA(n_components = n_components, whiten = True))\n",
    "train_embedding = lsa.fit_transform(train_dataset.embedding)\n",
    "test_embedding = lsa.transform(test_dataset.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 28.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models = []\n",
    "for i in trange(train_dataset.labels.shape[1]):\n",
    "    labels = train_dataset.labels[:, i] \n",
    "    clf    =  LogisticRegression(C = 0.1)\n",
    "    clf.fit(train_embedding, labels)\n",
    "    models.append(clf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 288.31it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = []\n",
    "for i in trange(test_dataset.labels.shape[1]):\n",
    "    prediction = models[i].predict_proba(test_embedding)[:, 1]\n",
    "    predictions.append(prediction)\n",
    "\n",
    "predictions = np.stack(predictions).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve:  0.93026706231454\n",
      "Result( 1254 / 1348 )\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "indices = predictions.argmax(axis = 1)\n",
    "tools   = [test_dataset.label_index[i] for i in indices]\n",
    "mapping = {test_dataset.instance_index[i]: tool for i, tool in enumerate(tools)}\n",
    "\n",
    "b1_result = evaluator.eval(mapping)\n",
    "\n",
    "print(\"Solve: \", b1_result.score() / test_dataset.embedding.shape[0])\n",
    "print(b1_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline #2: Predict PAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes = train_dataset.runtimes\n",
    "mask     = train_dataset.labels\n",
    "\n",
    "weight = (runtimes * mask) + ((1 - mask) * 9000)\n",
    "log_weight = np.log10(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.37it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "models = []\n",
    "for i in trange(train_dataset.labels.shape[1]):\n",
    "    labels = train_dataset.labels[:, i]\n",
    "    sample_weight = weight[:, i]\n",
    "    clf    =  RidgeCV(alphas = [1e-3, 1e-2, 1e-1, 1])\n",
    "    clf.fit(train_embedding, sample_weight)\n",
    "    models.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 181.35it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = []\n",
    "for i in trange(test_dataset.labels.shape[1]):\n",
    "    prediction = models[i].predict(test_embedding)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "predictions = np.stack(predictions).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve (sim):  0.9317507418397626\n",
      "Result( 1256 / 1348 )\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "indices = predictions.argmin(axis = 1)\n",
    "solve_mask = np.zeros(predictions.shape)\n",
    "solve_mask[np.arange(indices.shape[0]), indices] = 1\n",
    "\n",
    "solvabilty = (solve_mask * test_dataset.labels).max(axis = 1).mean()\n",
    "\n",
    "tools   = [test_dataset.label_index[i] for i in indices]\n",
    "mapping = {test_dataset.instance_index[i]: tool for i, tool in enumerate(tools)}\n",
    "\n",
    "b2_result = evaluator.eval(mapping)\n",
    "\n",
    "print(\"Solve (sim): \", b2_result.score() / test_dataset.embedding.shape[0])\n",
    "print(b2_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline #3: Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes = train_dataset.runtimes\n",
    "mask     = train_dataset.labels\n",
    "weight = (runtimes * mask) + ((1 - mask) * 9000)\n",
    "\n",
    "rank_pos    = []\n",
    "rank_weight = []\n",
    "\n",
    "for i in range(weight.shape[1] - 1):\n",
    "    for j in range(i + 1, weight.shape[1]):\n",
    "        rank_pos.append((i, j))\n",
    "        rank_weight.append((weight[:, j] - weight[:, i]))\n",
    "\n",
    "rank_weight = np.stack(rank_weight).transpose()\n",
    "rank_labels, rank_sample_weight = np.clip(np.sign(rank_weight), 0, 1), np.abs(rank_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:06<00:00,  2.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models = []\n",
    "for i in trange(rank_labels.shape[1]):\n",
    "    labels = rank_labels[:, i] \n",
    "    sample_weight = rank_sample_weight[:, i]\n",
    "\n",
    "    if labels.min() != labels.max():\n",
    "        clf    =  LogisticRegression(C = 0.1, max_iter = 10_000)\n",
    "        clf.fit(train_embedding, labels, sample_weight = sample_weight)\n",
    "    else:\n",
    "        clf = labels.min()\n",
    "\n",
    "    models.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 393.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1348, 15)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = []\n",
    "for i in trange(len(models)):\n",
    "    model = models[i]\n",
    "    try:\n",
    "        prediction = model.predict_proba(test_embedding)[:, 1]\n",
    "    except Exception:\n",
    "        prediction = np.array([model] * test_embedding.shape[0])\n",
    "    predictions.append(prediction)\n",
    "\n",
    "predictions = np.stack(predictions).transpose()\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros((test_dataset.labels.shape))\n",
    "for i, (p, k) in enumerate(rank_pos):\n",
    "    pred = predictions[:, i]\n",
    "\n",
    "    scores[:, p] += pred\n",
    "    scores[:, k] += (1 - pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve (b1):  0.93026706231454\n",
      "Solve (b2):  0.9317507418397626\n",
      "Solve (b3):  0.9287833827893175\n",
      "Result( 1252 / 1348 )\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "indices = scores.argmax(axis = 1)\n",
    "tools   = [test_dataset.label_index[i] for i in indices]\n",
    "mapping = {test_dataset.instance_index[i]: tool for i, tool in enumerate(tools)}\n",
    "\n",
    "b3_result = evaluator.eval(mapping)\n",
    "\n",
    "print(\"Solve (b1): \", b1_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b2): \", b2_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b3): \", b3_result.score() / test_dataset.embedding.shape[0])\n",
    "print(b3_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes = train_dataset.runtimes\n",
    "mask     = train_dataset.labels\n",
    "weight = (runtimes * mask) + ((1 - mask) * 9000)\n",
    "\n",
    "vbs = weight.min(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current regret: 8947.658623054735\n"
     ]
    }
   ],
   "source": [
    "tools    = []\n",
    "models   = []\n",
    "rank_pos = []\n",
    "regret = np.full((train_dataset.embedding.shape[0],), 9000)  - vbs\n",
    "print(\"Current regret:\", regret.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools: ['ki']\n",
      "Current regret: 1974.9199483111734\n"
     ]
    }
   ],
   "source": [
    "# Compute first model\n",
    "\n",
    "advantages = []\n",
    "for i in range(weight.shape[1]):\n",
    "    performance = weight[:, i]\n",
    "    cregret     = performance - vbs\n",
    "    advantage   = regret - cregret\n",
    "    advantages.append(advantage)\n",
    "\n",
    "candidate, advantage = max(enumerate(advantages), key = lambda x: x[1].mean())\n",
    "tools.append(candidate)\n",
    "regret = regret - advantage\n",
    "print(\"Tools:\", [train_dataset.label_index[i] for i in tools])\n",
    "print(\"Current regret:\", regret.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vaitp 1532.1492602958506\n"
     ]
    }
   ],
   "source": [
    "def max_corr(index):\n",
    "    compare_weights = np.stack([weight[:, index]] + [weight[:, i] for i in tools])\n",
    "    covariance = np.corrcoef(compare_weights)\n",
    "    index_cov  = covariance[0, 1:]\n",
    "    return index_cov.max()\n",
    "    \n",
    "\n",
    "advantages = []\n",
    "for i in range(weight.shape[1]):\n",
    "    if max_corr(i) >= 0.9:\n",
    "        advantages.append(np.array([0] * weight.shape[0]))\n",
    "        continue # This is not always possible (better option?)\n",
    "    performance = weight[:, i]\n",
    "    cregret     = performance - vbs\n",
    "    advantage   = regret - cregret\n",
    "    advantage   = np.clip(advantage, 0, None)\n",
    "    advantages.append(advantage)\n",
    "\n",
    "candidate, advantage = max(enumerate(advantages), key = lambda x: x[1].mean())\n",
    "print(train_dataset.label_index[candidate], advantage.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create classifier\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for p in tqdm(tools):\n",
    "    rank_pos.append((p, candidate))\n",
    "    advantage = weight[:, candidate] - weight[:, p]\n",
    "    labels, sample_weight = np.clip(np.sign(advantage), 0, 1), np.abs(advantage)\n",
    "    try:\n",
    "        clf = LogisticRegression(C = 0.1, max_iter = 10_000)\n",
    "        clf.fit(train_embedding, labels, sample_weight = sample_weight)\n",
    "    except Exception:\n",
    "        clf = labels.min()\n",
    "\n",
    "    models.append(clf)\n",
    "\n",
    "tools.append(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 115.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compute new predictions\n",
    "predictions = []\n",
    "for i in trange(len(models)):\n",
    "    model = models[i]\n",
    "    try:\n",
    "        prediction = model.predict_proba(train_embedding)[:, 1]\n",
    "    except Exception:\n",
    "        prediction = np.array([model] * train_embedding.shape[0])\n",
    "    predictions.append(prediction)\n",
    "\n",
    "predictions = np.stack(predictions).transpose()\n",
    "\n",
    "scores = np.zeros((weight.shape))\n",
    "for i, (p, k) in enumerate(rank_pos):\n",
    "    pred = predictions[:, i]\n",
    "\n",
    "    scores[:, p] += pred\n",
    "    scores[:, k] += (1 - pred)\n",
    "\n",
    "selection = scores.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools: ['ki', 'vaitp']\n",
      "Current regret: 642.6346333086556\n"
     ]
    }
   ],
   "source": [
    "selected_weight = weight[np.arange(selection.shape[0]), selection]\n",
    "regret = selected_weight - vbs\n",
    "print(\"Tools:\", [train_dataset.label_index[i] for i in tools])\n",
    "print(\"Current regret:\", np.mean(regret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 274.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve (b1):  0.93026706231454\n",
      "Solve (b2):  0.9317507418397626\n",
      "Solve (b3):  0.9287833827893175\n",
      "Solve (b4):  0.8887240356083086\n",
      "Result( 1198 / 1348 )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute new predictions\n",
    "predictions = []\n",
    "for i in trange(len(models)):\n",
    "    model = models[i]\n",
    "    try:\n",
    "        prediction = model.predict_proba(test_embedding)[:, 1]\n",
    "    except Exception:\n",
    "        prediction = np.array([model] * test_embedding.shape[0])\n",
    "    predictions.append(prediction)\n",
    "\n",
    "predictions = np.stack(predictions).transpose()\n",
    "\n",
    "scores = np.zeros((test_dataset.labels.shape))\n",
    "for i, (p, k) in enumerate(rank_pos):\n",
    "    pred = predictions[:, i]\n",
    "\n",
    "    scores[:, p] += pred\n",
    "    scores[:, k] += (1 - pred)\n",
    "\n",
    "selection = scores.argmax(axis = 1)\n",
    "\n",
    "# Evaluate\n",
    "indices = scores.argmax(axis = 1)\n",
    "solve_mask = np.zeros(scores.shape)\n",
    "solve_mask[np.arange(indices.shape[0]), indices] = 1\n",
    "\n",
    "tool_names   = [test_dataset.label_index[i] for i in indices]\n",
    "mapping = {test_dataset.instance_index[i]: tool for i, tool in enumerate(tool_names)}\n",
    "\n",
    "b4_result = evaluator.eval(mapping)\n",
    "\n",
    "print(\"Solve (b1): \", b1_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b2): \", b2_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b3): \", b3_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b4): \", b4_result.score() / test_dataset.embedding.shape[0])\n",
    "print(b4_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline #5: Sequential compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pa', 60), ('bmc', 180), ('vaitp', 480), ('ki', 900)]\n"
     ]
    }
   ],
   "source": [
    "from pesco.optim import optimize_portfolio\n",
    "\n",
    "seqp = optimize_portfolio(train_dataset.labels, train_dataset.runtimes)\n",
    "seqp = [(train_dataset.label_index[solver], runtime) for solver, runtime in seqp.portfolio]\n",
    "print(seqp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve (b1):  0.93026706231454\n",
      "Solve (b2):  0.9317507418397626\n",
      "Solve (b3):  0.9287833827893175\n",
      "Solve (b4):  0.8887240356083086\n",
      "Solve (b5):  0.9339762611275965\n",
      "Result( 1259 / 1348 )\n"
     ]
    }
   ],
   "source": [
    "mapping = {instance: seqp for instance in test_dataset.instance_index}\n",
    "b5_result = evaluator.eval(mapping)\n",
    "\n",
    "print(\"Solve (b1): \", b1_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b2): \", b2_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b3): \", b3_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b4): \", b4_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b5): \", b5_result.score() / test_dataset.embedding.shape[0])\n",
    "print(b5_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline #6: Selection of Sequential Compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes = train_dataset.runtimes\n",
    "mask     = train_dataset.labels\n",
    "weight = (runtimes * mask) + ((1 - mask) * 9000)\n",
    "\n",
    "vbs = weight.min(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current regret: 8947.658623054735\n"
     ]
    }
   ],
   "source": [
    "tools    = []\n",
    "tool_weights = []\n",
    "\n",
    "models   = []\n",
    "rank_pos = []\n",
    "regret = np.full((train_dataset.embedding.shape[0],), 9000)  - vbs\n",
    "print(\"Current regret:\", regret.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pa:60,bmc:180,vaitp:480,ki:900'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "seqp = optimize_portfolio(train_dataset.labels, train_dataset.runtimes, penalty = regret)\n",
    "seqp = [(train_dataset.label_index[solver], runtime) for solver, runtime in seqp.portfolio]\n",
    "tool = \",\".join(\":\".join([str(_b) for _b in b]) for b in seqp)\n",
    "tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current regret: 641.4163920719972\n"
     ]
    }
   ],
   "source": [
    "# Eval tool\n",
    "mapping = {instance: seqp for instance in train_dataset.instance_index}\n",
    "result  = evaluator.eval(mapping)\n",
    "tool_solve = [r[1].startswith(str(r[2]).lower()) for r in result]\n",
    "tool_solve = np.array([1 if t else 0 for t in tool_solve])\n",
    "tool_runtimes = np.array([r[3] for r in result])\n",
    "tool_weight   = tool_runtimes * tool_solve + 9000 * (1 - tool_solve)\n",
    "regret = tool_weight - vbs\n",
    "\n",
    "tools.append(tool)\n",
    "tool_weights.append(tool_weight)\n",
    "\n",
    "print(\"Current regret:\", regret.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ki:900'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqp = optimize_portfolio(train_dataset.labels, train_dataset.runtimes, penalty = regret)\n",
    "seqp = [(train_dataset.label_index[solver], runtime) for solver, runtime in seqp.portfolio]\n",
    "tool = \",\".join(\":\".join([str(_b) for _b in b]) for b in seqp)\n",
    "tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.45146552e+00, 4.73117862e+00, 9.00000000e+03, ...,\n",
       "       4.10204674e+01, 4.24808199e+01, 1.08716914e+01])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {instance: seqp for instance in train_dataset.instance_index}\n",
    "result  = evaluator.eval(mapping)\n",
    "tool_solve = [r[1].startswith(str(r[2]).lower()) for r in result]\n",
    "tool_solve = np.array([1 if t else 0 for t in tool_solve])\n",
    "tool_runtimes = np.array([r[3] for r in result])\n",
    "tool_weight   = tool_runtimes * tool_solve + 9000 * (1 - tool_solve)\n",
    "tool_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_weights = np.stack([tool_weight] + [weight for weight in tool_weights])\n",
    "covariance = np.corrcoef(compare_weights)\n",
    "index_cov  = covariance[0, 1:]\n",
    "index_cov.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:01<00:00,  2.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create classifier\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, p in enumerate(tqdm(tools)):\n",
    "    rank_pos.append((i, len(tools)))\n",
    "    advantage = tool_weight - tool_weights[i]\n",
    "    labels, sample_weight = np.clip(np.sign(advantage), 0, 1), np.abs(advantage)\n",
    "    try:\n",
    "        clf = LogisticRegression(C = 0.1, max_iter = 10_000)\n",
    "        clf.fit(train_embedding, labels, sample_weight = sample_weight)\n",
    "    except Exception:\n",
    "        clf = labels.min()\n",
    "\n",
    "    models.append(clf)\n",
    "\n",
    "tools.append(tool)\n",
    "tool_weights.append(tool_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 267.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Compute new predictions\n",
    "predictions = []\n",
    "for i in trange(len(models)):\n",
    "    model = models[i]\n",
    "    try:\n",
    "        prediction = model.predict_proba(train_embedding)[:, 1]\n",
    "    except Exception:\n",
    "        prediction = np.array([model] * train_embedding.shape[0])\n",
    "    predictions.append(prediction)\n",
    "\n",
    "predictions = np.stack(predictions).transpose()\n",
    "\n",
    "scores = np.zeros((predictions.shape[0], len(tools)))\n",
    "for i, (p, k) in enumerate(rank_pos):\n",
    "    pred = predictions[:, i]\n",
    "\n",
    "    scores[:, p] += pred\n",
    "    scores[:, k] += (1 - pred)\n",
    "\n",
    "selection = scores.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools: ['pa:60,bmc:180,vaitp:480,ki:900', 'va:420,ki:420,symbolic:900', 'ki:900', 'vaitp:840,bmc:900', 'symbolic:120,pa:480,bmc:900']\n",
      "Current regret: 88.09078617217578\n"
     ]
    }
   ],
   "source": [
    "cweight = np.stack(tool_weights).transpose()\n",
    "selected_weight = cweight[np.arange(selection.shape[0]), selection]\n",
    "regret = selected_weight - vbs\n",
    "print(\"Tools:\", tools)\n",
    "print(\"Current regret:\", np.mean(regret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 341.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve (b1):  0.93026706231454\n",
      "Solve (b2):  0.9317507418397626\n",
      "Solve (b3):  0.9287833827893175\n",
      "Solve (b4):  0.8887240356083086\n",
      "Solve (b5):  0.9339762611275965\n",
      "Solve (b6):  0.9673590504451038\n",
      "Result( 1304 / 1348 )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute new predictions\n",
    "predictions = []\n",
    "for i in trange(len(models)):\n",
    "    model = models[i]\n",
    "    try:\n",
    "        prediction = model.predict_proba(test_embedding)[:, 1]\n",
    "    except Exception:\n",
    "        prediction = np.array([model] * test_embedding.shape[0])\n",
    "    predictions.append(prediction)\n",
    "\n",
    "predictions = np.stack(predictions).transpose()\n",
    "\n",
    "scores = np.zeros((predictions.shape[0], len(tools)))\n",
    "for i, (p, k) in enumerate(rank_pos):\n",
    "    pred = predictions[:, i]\n",
    "\n",
    "    scores[:, p] += pred\n",
    "    scores[:, k] += (1 - pred)\n",
    "\n",
    "selection = scores.argmax(axis = 1)\n",
    "\n",
    "# Evaluate\n",
    "tool_names   = [tools[i] for i in selection]\n",
    "mapping = {test_dataset.instance_index[i]: tool for i, tool in enumerate(tool_names)}\n",
    "\n",
    "b6_result = evaluator.eval(mapping)\n",
    "\n",
    "print(\"Solve (b1): \", b1_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b2): \", b2_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b3): \", b3_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b4): \", b4_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b5): \", b5_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b6): \", b6_result.score() / test_dataset.embedding.shape[0])\n",
    "print(b6_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline #7: k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "K = 4\n",
    "cluster_assign = KMeans(n_clusters = K).fit_predict(train_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bmc:900\n",
      "ki:60,pa:60,symbolic:180,va:240,bmc:300,vaitp:900\n",
      "ki:120,symbolic:240,vaitp:480,pa:900\n",
      "bmc:60,pa:60,ki:180,va:300,vaitp:900\n"
     ]
    }
   ],
   "source": [
    "tools    = []\n",
    "tool_weights = []\n",
    "\n",
    "for k in range(K):\n",
    "    index = cluster_assign == k\n",
    "    labels = train_dataset.labels[index]\n",
    "    runtimes = train_dataset.runtimes[index]\n",
    "    seqp = optimize_portfolio(labels, runtimes)\n",
    "    seqp = [(train_dataset.label_index[solver], runtime) for solver, runtime in seqp.portfolio]\n",
    "    tool = \",\".join(\":\".join([str(_b) for _b in b]) for b in seqp)\n",
    "    print(tool)\n",
    "\n",
    "    mapping = {instance: seqp for instance in train_dataset.instance_index}\n",
    "    result  = evaluator.eval(mapping)\n",
    "    tool_solve = [r[1].startswith(str(r[2]).lower()) for r in result]\n",
    "    tool_solve = np.array([1 if t else 0 for t in tool_solve])\n",
    "    tool_runtimes = np.array([r[3] for r in result])\n",
    "    tool_weight   = tool_runtimes * tool_solve + 9000 * (1 - tool_solve)\n",
    "\n",
    "    tools.append(tool)\n",
    "    tool_weights.append(tool_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool in train_dataset.label_index:\n",
    "    mapping = {instance: tool for instance in train_dataset.instance_index}\n",
    "    result  = evaluator.eval(mapping)\n",
    "    tool_solve = [r[1].startswith(str(r[2]).lower()) for r in result]\n",
    "    tool_solve = np.array([1 if t else 0 for t in tool_solve])\n",
    "    tool_runtimes = np.array([r[3] for r in result])\n",
    "    tool_weight   = tool_runtimes * tool_solve + 9000 * (1 - tool_solve)\n",
    "\n",
    "    tools.append(tool)\n",
    "    tool_weights.append(tool_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_pos    = []\n",
    "rank_weight = []\n",
    "\n",
    "for i in range(len(tools) - 1):\n",
    "    for j in range(i + 1, len(tools)):\n",
    "        rank_pos.append((i, j))\n",
    "        rank_weight.append((tool_weights[j] - tool_weights[i]))\n",
    "\n",
    "rank_weight = np.stack(rank_weight).transpose()\n",
    "rank_labels, rank_sample_weight = np.clip(np.sign(rank_weight), 0, 1), np.abs(rank_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:17<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models = []\n",
    "for i in trange(rank_labels.shape[1]):\n",
    "    labels = rank_labels[:, i] \n",
    "    sample_weight = rank_sample_weight[:, i]\n",
    "\n",
    "    if labels.min() != labels.max():\n",
    "        clf    =  LogisticRegression(C = 0.1, max_iter = 10_000)\n",
    "        clf.fit(train_embedding, labels, sample_weight = sample_weight)\n",
    "    else:\n",
    "        clf = labels.min()\n",
    "\n",
    "    models.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 602.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3, 4, 4, ..., 9, 6, 3])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute new predictions\n",
    "predictions = []\n",
    "for i in trange(len(models)):\n",
    "    model = models[i]\n",
    "    try:\n",
    "        prediction = model.predict_proba(test_embedding)[:, 1]\n",
    "    except Exception:\n",
    "        prediction = np.array([model] * test_embedding.shape[0])\n",
    "    predictions.append(prediction)\n",
    "\n",
    "predictions = np.stack(predictions).transpose()\n",
    "\n",
    "scores = np.zeros((predictions.shape[0], len(tools)))\n",
    "for i, (p, k) in enumerate(rank_pos):\n",
    "    pred = predictions[:, i]\n",
    "\n",
    "    scores[:, p] += pred\n",
    "    scores[:, k] += (1 - pred)\n",
    "\n",
    "selection = scores.argmax(axis = 1)\n",
    "selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve (b1):  0.93026706231454\n",
      "Solve (b2):  0.9317507418397626\n",
      "Solve (b3):  0.9287833827893175\n",
      "Solve (b4):  0.8887240356083086\n",
      "Solve (b5):  0.9339762611275965\n",
      "Solve (b6):  0.9673590504451038\n",
      "Solve (b7):  0.9658753709198813\n",
      "Result( 1302 / 1348 )\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "tool_names   = [tools[i] for i in selection]\n",
    "mapping = {test_dataset.instance_index[i]: tool for i, tool in enumerate(tool_names)}\n",
    "\n",
    "b7_result = evaluator.eval(mapping)\n",
    "\n",
    "print(\"Solve (b1): \", b1_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b2): \", b2_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b3): \", b3_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b4): \", b4_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b5): \", b5_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b6): \", b6_result.score() / test_dataset.embedding.shape[0])\n",
    "print(\"Solve (b7): \", b7_result.score() / test_dataset.embedding.shape[0])\n",
    "print(b7_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e49e4b1338c536a3b24fc51f719eff5e9bb6f42833c3172f334ab85d121b9a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
