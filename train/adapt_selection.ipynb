{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "PATH = \"results/predict_single_config/predict_benchmark.2022-11-21_13-23-34.files/\"\n",
    "\n",
    "prediction_files = glob(os.path.join(PATH, \"**\", \"prediction.json\"), recursive = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_index = []\n",
    "label_index    = {}\n",
    "\n",
    "predictions = []\n",
    "\n",
    "import json\n",
    "\n",
    "for file in prediction_files:\n",
    "    name = os.path.basename(os.path.dirname(os.path.dirname(file)))\n",
    "    \n",
    "    with open(file, 'r') as i:\n",
    "        prediction = json.load(i)\n",
    "    \n",
    "    for tool_name, pred in prediction.items():\n",
    "        if tool_name == \"features\": continue\n",
    "        if tool_name not in label_index: label_index[tool_name] = len(label_index)\n",
    "    \n",
    "    output = [0] * len(label_index)\n",
    "    for tool_name, pred in prediction.items():\n",
    "        if tool_name == \"features\": continue\n",
    "        output[label_index[tool_name]] = pred\n",
    "    \n",
    "    predictions.append(output)\n",
    "    instance_index.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6494, 385)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pesco.data.utils import load_dataset\n",
    "dataset = load_dataset(\"../pesco_data/datasets/svcomp22_count_embedding_new.jsonl\", \"../pesco_data/cpachecker_labels_nobam.jsonl\", fill_unknown = True)\n",
    "dataset.embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_prediction = [None] * dataset.embedding.shape[0]\n",
    "\n",
    "instance_lookup = {k.split(\"/\")[1]: i for i, k in enumerate(dataset.instance_index)}\n",
    "\n",
    "count = 0\n",
    "for i, name in enumerate(instance_index):\n",
    "    name = name.replace(\".yml\", \"\").replace(\".c\", \"\").replace(\".i\", \"\")\n",
    "    if name in instance_lookup:\n",
    "        aligned_prediction[instance_lookup[name]] = predictions[i]\n",
    "\n",
    "import numpy as np\n",
    "aligned_mask       = np.array([aligned_prediction[i] is not None for i in range(dataset.embedding.shape[0])])\n",
    "aligned_prediction = np.array([p for p in aligned_prediction if p is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bmc': 0, 'ki': 1, 'pa': 2, 'symbolic': 3, 'va': 4, 'vaitp': 5}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_entries = [label_index[f\"{k}\"] for k in dataset.label_index]\n",
    "aligned_prediction = [aligned_prediction[:, i] for i in allowed_entries]\n",
    "aligned_prediction = np.stack(aligned_prediction).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5568, 6)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, runtimes = dataset.labels[aligned_mask], dataset.runtimes[aligned_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5568, 6), (5568, 6), (5568, 6))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_prediction.shape, labels.shape, runtimes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bmc 501 / 508 Times selected: 736\n",
      "ki 1288 / 1307 Times selected: 1756\n",
      "pa 388 / 438 Times selected: 589\n",
      "symbolic 484 / 501 Times selected: 707\n",
      "va 181 / 182 Times selected: 233\n",
      "vaitp 810 / 839 Times selected: 1547\n",
      "Score before: 3652.0 926 4578.0\n",
      "Max solvable 4563\n"
     ]
    }
   ],
   "source": [
    "def _evaluate(candidate, y, runtimes):\n",
    "    if isinstance(candidate, int):\n",
    "        candidate = [(candidate, 900)]\n",
    "\n",
    "    eval_results   = np.zeros((y.shape[0],))\n",
    "    eval_runtimes  = np.zeros((y.shape[0],)) \n",
    "\n",
    "    running = np.ones((y.shape[0],))\n",
    "    for tool, timelimit in candidate:\n",
    "        labels, truntimes = y[:, tool], runtimes[:, tool]\n",
    "        timemask = np.clip(timelimit - truntimes + 1, 0, 1).astype(int)\n",
    "\n",
    "        cruntimes = timemask * truntimes + (1 - timemask) * timelimit\n",
    "\n",
    "        eval_results  = (1 - running) * eval_results + running * timemask * labels\n",
    "        eval_runtimes = (1 - running) * eval_runtimes + running * (eval_runtimes + cruntimes)\n",
    "        running -= running * timemask * labels\n",
    "\n",
    "    return eval_results, eval_runtimes\n",
    "\n",
    "lookup = {v: k for k, v in label_index.items()}\n",
    "aligned_selection = aligned_prediction.argmin(axis = 1)\n",
    "\n",
    "solved = 0\n",
    "\n",
    "for k in lookup.keys():\n",
    "    index = aligned_selection == k\n",
    "    if not np.any(index): continue\n",
    "\n",
    "    _labels   = labels[index]\n",
    "    _runtimes = runtimes[index]\n",
    "\n",
    "    candidate = lookup[k]\n",
    "    \n",
    "    if \",\" in candidate:\n",
    "        candidate = candidate.split(\",\")\n",
    "    else:\n",
    "        candidate = [candidate]\n",
    "    \n",
    "    candidate = [x.split(\":\") if \":\" in x else (x, 900) for x in candidate]\n",
    "    candidate = [(dataset.label_index.index(x[0]), int(x[1])) for x in candidate]\n",
    "    result, _ = _evaluate(candidate, _labels, _runtimes)\n",
    "    _solved    = result.sum()\n",
    "    _solvable  = _labels.max(axis = 1).sum()\n",
    "    print(lookup[k], int(_solved), \"/\", int(_solvable), \"Times selected:\", _labels.shape[0])\n",
    "    solved += _solved\n",
    "\n",
    "pre_solved = dataset.labels.shape[0] - aligned_prediction.shape[0]\n",
    "print(\"Score before:\", solved, pre_solved, solved + pre_solved)\n",
    "print(\"Max solvable\", dataset.labels.max(axis = 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score before: 3652 926 4578\n",
      "Max solvable 4563\n"
     ]
    }
   ],
   "source": [
    "aligned_selection = aligned_prediction.argmin(axis = 1)\n",
    "selected_entries  = labels[np.arange(labels.shape[0]), aligned_selection]\n",
    "solved = dataset.labels.shape[0] - aligned_prediction.shape[0]\n",
    "print(\"Score before:\", selected_entries.sum(), solved, selected_entries.sum() + solved)\n",
    "print(\"Max solvable\", dataset.labels.max(axis = 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bmc', 'ki', 'va', 'vaitp']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = aligned_prediction\n",
    "subset_size = 4\n",
    "num_verifier = scores.shape[1]\n",
    "best_mask    = np.zeros(num_verifier)\n",
    "best_score   = 0\n",
    "\n",
    "for _ in range(num_verifier):\n",
    "    current_mask  = None\n",
    "    current_score = 0\n",
    "\n",
    "    for cand in range(num_verifier):\n",
    "        if best_mask[cand] == 1: continue\n",
    "\n",
    "        test_mask = np.copy(best_mask)\n",
    "        test_mask[cand] = 1\n",
    "        test_mask_b   = np.broadcast_to(test_mask, scores.shape)\n",
    "        test_scores = (scores * test_mask_b) + ((1 - test_mask_b) * np.max(scores)) \n",
    "        test_selection = test_scores.argmin(axis = 1)\n",
    "        test_score  = labels[np.arange(labels.shape[0]), test_selection].mean()\n",
    "\n",
    "        if test_score > current_score:\n",
    "            current_mask = test_mask\n",
    "            current_score = test_score\n",
    "\n",
    "    if current_score > best_score:\n",
    "        best_score = current_score\n",
    "        best_mask  = current_mask\n",
    "\n",
    "        if subset_size != -1 and best_mask.sum() >= subset_size:\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "[l for i, l in enumerate(dataset.label_index) if best_mask[i] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mask = np.ones((labels.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 5, 5, 5])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_scores = best_mask * scores + (1 - best_mask) * 9000\n",
    "masked_selection = masked_scores.argmin(axis = 1)\n",
    "masked_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Times selected: 826\n",
      "Unsolved for k = 0: 0 / 15\n",
      "Times selected: 2054\n",
      "Unsolved for k = 1: 6 / 53\n",
      "Times selected: 431\n",
      "Unsolved for k = 4: 1 / 4\n",
      "Times selected: 2257\n",
      "Unsolved for k = 5: 37 / 76\n",
      "Solved tasks: 3731 926 4657\n",
      "bmc --> [('vaitp', 60), ('symbolic', 60), ('pa', 60), ('bmc', 900)]\n",
      "ki --> [('pa', 60), ('bmc', 60), ('symbolic', 60), ('ki', 900)]\n",
      "va --> [('pa', 60), ('va', 900)]\n",
      "vaitp --> [('symbolic', 60), ('pa', 60), ('vaitp', 900)]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pesco.optim import optimize_portfolio\n",
    "\n",
    "cluster_assign = masked_selection\n",
    "\n",
    "_tools = []\n",
    "\n",
    "solved = 0\n",
    "solvables = labels.max(axis = 1) == 1\n",
    "\n",
    "for k in range(labels.shape[1]):\n",
    "    index = cluster_assign == k\n",
    "    if np.any(index):\n",
    "        _labels = labels[index]\n",
    "        truntimes = runtimes[index]\n",
    "\n",
    "        print(\"Times selected:\", _labels.shape[0])\n",
    "\n",
    "        solvable = _labels.max(axis = 1) == 1\n",
    "        _labels  = _labels[solvable]\n",
    "        truntimes = truntimes[solvable]\n",
    "\n",
    "        unsolved = (1 - _labels[:, k]).sum()\n",
    "\n",
    "        candidate = optimize_portfolio(_labels, truntimes, max_runtime = 900, q = 60)\n",
    "        solved += _labels.shape[0] - len(candidate.unsolved)\n",
    "        print(\"Unsolved for k = %d:\" % k, len(candidate.unsolved), \"/\", unsolved)\n",
    "        _tools.append(candidate.portfolio)\n",
    "    else:\n",
    "        _tools.append(((k, 900),))\n",
    "\n",
    "pre_solved = dataset.labels.shape[0] - aligned_prediction.shape[0]\n",
    "print(\"Solved tasks:\", solved, pre_solved, solved + pre_solved)\n",
    "\n",
    "for i, tool in enumerate(_tools):\n",
    "    if best_mask[i] == 0: continue\n",
    "    print(dataset.label_index[i], \"-->\", [(dataset.label_index[a], b) for a, b in tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((5, 60), (3, 60), (2, 60), (0, 900)),\n",
       " ((2, 60), (3, 60), (0, 200), (1, 900)),\n",
       " ((2, 900),),\n",
       " ((3, 900),),\n",
       " ((2, 60), (4, 900)),\n",
       " ((3, 60), (2, 60), (5, 900))]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tools\n",
    "n_tools = list(_tools)\n",
    "n_tools[1] = ((2, 60), (3, 60), (0, 200), (1, 900))\n",
    "n_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vaitp', 60), ('symbolic', 60), ('pa', 60), ('bmc', 900)] 545 / 545\n",
      "[('pa', 60), ('bmc', 60), ('symbolic', 60), ('ki', 900)] 1507 / 1513\n",
      "[('pa', 60), ('va', 900)] 350 / 351\n",
      "[('symbolic', 60), ('pa', 60), ('vaitp', 900)] 1332 / 1366\n",
      "Score before: 3734.0 926 4660.0\n",
      "Max solvable 4563\n"
     ]
    }
   ],
   "source": [
    "def _evaluate(candidate, y, runtimes):\n",
    "    if isinstance(candidate, int):\n",
    "        candidate = [(candidate, 900)]\n",
    "\n",
    "    eval_results   = np.zeros((y.shape[0],))\n",
    "    eval_runtimes  = np.zeros((y.shape[0],)) \n",
    "\n",
    "    running = np.ones((y.shape[0],))\n",
    "    for tool, timelimit in candidate:\n",
    "        labels, truntimes = y[:, tool], runtimes[:, tool]\n",
    "        timemask = np.clip(timelimit - truntimes + 1, 0, 1).astype(int)\n",
    "\n",
    "        cruntimes = timemask * truntimes + (1 - timemask) * timelimit\n",
    "\n",
    "        eval_results  = (1 - running) * eval_results + running * timemask * labels\n",
    "        eval_runtimes = (1 - running) * eval_runtimes + running * (eval_runtimes + cruntimes)\n",
    "        running -= running * timemask * labels\n",
    "\n",
    "    return eval_results, eval_runtimes\n",
    "\n",
    "solved = 0\n",
    "\n",
    "for k in range(labels.shape[1]):\n",
    "    index = masked_selection == k\n",
    "    if not np.any(index): continue\n",
    "\n",
    "    _labels   = labels[index]\n",
    "    _runtimes = runtimes[index]\n",
    "\n",
    "    solvable  = _labels.max(axis = 1) == 1\n",
    "    _labels   = _labels[solvable]\n",
    "    _runtimes = _runtimes[solvable]\n",
    "\n",
    "    candidate = _tools[k]\n",
    "    result, _ = _evaluate(candidate, _labels, _runtimes)\n",
    "    _solved    = result.sum()\n",
    "    _solvable  = _labels.max(axis = 1).sum()\n",
    "\n",
    "    named_cand = [(lookup[x], y) for x, y in candidate]\n",
    "    print(named_cand, int(_solved), \"/\", int(_solvable))\n",
    "    solved += _solved\n",
    "\n",
    "pre_solved = dataset.labels.shape[0] - aligned_prediction.shape[0]\n",
    "print(\"Score before:\", solved, pre_solved, solved + pre_solved)\n",
    "print(\"Max solvable\", dataset.labels.max(axis = 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bmc': 'vaitp:60,symbolic:60,pa:60,bmc:900',\n",
       " 'ki': 'pa:60,symbolic:60,ki:900',\n",
       " 'pa': 'passthrough',\n",
       " 'symbolic': 'passthrough',\n",
       " 'va': 'pa:60,va:900',\n",
       " 'vaitp': 'symbolic:60,pa:60,vaitp:900'}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {}\n",
    "\n",
    "for i, tool in enumerate(_tools):\n",
    "    label = dataset.label_index[i]\n",
    "    if best_mask[i] == 0:\n",
    "        mapping[label] = \"passthrough\"\n",
    "    else:\n",
    "        mapping[label] = \",\".join(f\"{dataset.label_index[_tool]}:{_time}\" for _tool, _time in tool)\n",
    "\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mapping.json\", \"w\") as o:\n",
    "    json.dump(mapping, o, indent = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ca9c187077675d143f74a1caf4c8baa54f8cc23c16765589074b4a31043f7b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
